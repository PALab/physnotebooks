{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geophysics 310 Lab 1: Python, Linear Regression, and Sea Level Rise \n",
    "\n",
    "The aim of this first 310 lab is to become familiar with using Python to analyse and visualise scientific data. We will also fit some models to sea level data to explain trends over time. \n",
    "\n",
    "**Lab work and submission:** Complete all the exercises within the notebook itself, unless otherwise stated. You are encouraged to work on the problems together, but everyone will submit their own work. Submit your .ipynb notebook file along with any other files or data through Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra is the topic of Chapter 12 of [A Guided Tour of Mathematical Methods for the Physical Sciences](http://www.cambridge.org/nz/academic/subjects/physics/mathematical-methods/guided-tour-mathematical-methods-physical-sciences-3rd-edition#KUoGXYx5FTwytcUg.97). In this notebook we treat linear regression on sea level measurements in the Auckland harbour as an application of linear algebra, as we solve the normal equations that describe linear regression. In the process, this notebook also has bearing on Inverse Problems (Chapter 22), and Statistics (Chapter 21). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the python libraries we'll use in this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # numerical tools\n",
    "import scipy.stats # linear regression function\n",
    "import scipy.optimize # curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea Level Data ###\n",
    "Sea level measurements for the Auckland harbour can be downloaded [here](https://auckland.figshare.com/ndownloader/files/21882006) (thanks to Dr. John Hannah). The value for each year is the average of many measurements taken throughout that year to obtain a mean value and standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read in our data. The Python library [pandas](https://pandas.pydata.org) can easily read CSV data for us, without needing to know a lot of programming. Then we will convert the *pandas* data into a [NumPy](http://numpy.org) matrix, so we can easily work with the data throughout the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harbour_data = pd.read_csv(\"https://auckland.figshare.com/ndownloader/files/21882006\")\n",
    "print(harbour_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this pandas data frame into a matrix that is easy to slice up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harbour_data_matrix = harbour_data.as_matrix()\n",
    "# check shape of matrix\n",
    "print(harbour_data_matrix.shape)\n",
    "# print tenth row\n",
    "print(harbour_data_matrix[9]) # remember, 0 is first row\n",
    "# print first column\n",
    "print(harbour_data_matrix[:,0]) #format for indexing is [rows,columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the matrix has all the time data in the first column, all the sea level height in the second column, and the standard deviations in the third column. In the cell below, create three variables ```time```, ```height```, and ```errors```, and assign the correct data to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time   =  #Extract the time data\n",
    "height =  #Extract the height data\n",
    "errors =  #Extract the standard deviations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot sea level in the Auckland Harbour, as a function of time, using [matplotlib](https://matplotlib.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.errorbar(time, height, yerr=errors, color='r', ecolor='gray', marker='o', linestyle='')\n",
    "plt.grid()\n",
    "plt.xlabel('Date (year)')\n",
    "plt.ylabel('Mean Sea Level (m)')\n",
    "plt.title('Princes Wharf, Auckland (New Zealand)')\n",
    "plt.axis('tight')\n",
    "plt.savefig('test.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression ###\n",
    "There is an obvious spread in these measurements. You can read about the challenges of tidal gauge readings [here](http://www.fig.net/resources/monthly_articles/2010/hannah_july_2010.asp). Nevertheless, a trend of increasing water depth seems clear. Next, we'll attempt to answer the question: \"What is the best fitting linear equation to these data?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, in Chapter 22, we will discuss Inverse Problems, where we address important questions about what it means to fit the data. Here, we are more concerned with minimizing the misfit between a vector of $n$ data points, $\\mathbf{d}$, and those predicted by a model $\\mathbf{m}$ that is represented by 2 variables: the slope and intercept of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we accept that these data can be represented by a straight line, then any datum at time $t$ would have a water depth $d = intercept + slope*t$. This would be true for all data, so we can write this in matrix form. If\n",
    "\n",
    "$$ \\mathbf{A}\\mathbf{m}= \\mathbf{d},$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix}1 & t_1 \\\\ \\vdots & \\vdots\\\\ 1& t_n\\end{pmatrix}, \\mathbf{m} = \\begin{pmatrix} \\textrm{intercept} \\\\ \\textrm{slope} \\end{pmatrix}, \\textrm{ and } \\mathbf{d} = \\begin{pmatrix} d_1 \\\\ \\vdots\\\\ d_n\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equations ###\n",
    "\n",
    "Finding the slope and intercept that best fit the data in a least--squares sense is derived in many text books, including Section~22.2 of ours. Suffices here to say that we want to manipulate the linear system of equations so that we \"free up\" $\\mathbf{m}$. If $\\mathbf{A}$ had an inverse, we could multiply the left and right of $ \\mathbf{A}\\mathbf{m}= \\mathbf{d}$ to achieve our goals. But $\\mathbf{A}$ is not even square, so there is no chance of that! The next best scenario is to multiply left and right side of  $\\mathbf{A}\\mathbf{m}= \\mathbf{d}$ by the transpose of $\\mathbf{A}$:\n",
    "\n",
    "$$ \\mathbf{A}^T\\mathbf{A}\\mathbf{m} = \\mathbf{A}^T\\mathbf{d}.$$ \n",
    "\n",
    "These are the so-called *normal equations*, and we can rewrite this system as\n",
    "\n",
    "$$ \\tilde{\\mathbf{A}}\\mathbf{m} = \\tilde{\\mathbf{d}},$$\n",
    "\n",
    "where $ \\tilde{\\mathbf{A}} = \\mathbf{A}^T\\mathbf{A}$ and $ \\tilde{\\mathbf{d}}= \\mathbf{A}^T\\mathbf{d}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best straight line through a set of points\n",
    "\n",
    "Here, we have a set of $n$ data points $\\left(x_{i},y_{i}\\right) $ for which we assume a linear relationship $y=a+bx$ is\n",
    "expected to hold between the variables. The data points do not lie exactly on this line due to measurement errors, and we would  like to find the best values of $a$ and $b$ given the data. The\n",
    "classical least-squares approach is to assume that the $x_{i}$ are\n",
    "known exactly. With this assumption, the aim is to find a straight\n",
    "line $\\widehat{y}_{i}=a+bx_{i}$ that approximates to the $y_{i}$ such that the\n",
    "value of the sum of the squared deviations\n",
    "\n",
    "$$\\varepsilon\\left( a,b\\right) =\\left( \\widehat{y}_{1}-y_{1}\\right)\n",
    "^{2}+\\left( \\widehat{y}_{2}-y_{2}\\right) ^{2}+\\ldots+\\left( \\widehat{y}\n",
    "_{n}-y_{n}\\right) ^{2}$$\n",
    "\n",
    "is minimized over all $a$ and $b$. $\\varepsilon\\left( a,b\\right) $ is\n",
    "called *the misfit function*. We know from calculus that we can find the minimum of a function by differentiating and setting to zero. If we differentiate $\\varepsilon\\left( a,b\\right) $ with respect to each of its variables and set these to zero, we find that the miminum of the cost function can be experssed as:\n",
    "\n",
    "$$ \\left(\n",
    "      \\begin{array}\n",
    "        [c]{cc}%\n",
    "        \\sum_{i=1}^{n}x_{i}^{2} & \\sum_{i=1}^{n}x_{i}\\\\\n",
    "        \\sum_{i=1}^{n}x_{i} & n\n",
    "      \\end{array}\n",
    "    \\right) \\left(\n",
    "      \\begin{array}\n",
    "        [c]{c}%\n",
    "        b\\\\\n",
    "        a\n",
    "      \\end{array}\n",
    "    \\right) =\\left(\n",
    "      \\begin{array}\n",
    "        [c]{c}%\n",
    "        \\sum_{i=1}^{n}x_{i}y_{i}\\\\\n",
    "        \\sum_{i=1}^{n}y_{i}%\n",
    "      \\end{array}\n",
    "    \\right)$$\n",
    "\n",
    "Note that this new system is the equivalent of $$ \\tilde{\\mathbf{A}}\\mathbf{m} = \\tilde{\\mathbf{d}},$$\n",
    "\n",
    "where $ \\tilde{\\mathbf{A}} = \\mathbf{A}^T\\mathbf{A}$ and $ \\tilde{\\mathbf{d}}= \\mathbf{A}^T\\mathbf{d}$.\n",
    "\n",
    "Python has many ways to solve this system of equations. In the following function definition, add lines to compute the elements of the matrix $\\mathbf{A}$ and the vector $d$. Then use $\\mathbf{A}$ and $d$ with the ```np.linalg.solve()``` function to find the intercept and slope of our best-fit line. You may find the ```np.dot()``` ([see here](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.dot.html)) and ```np.sum()``` ([see here](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.sum.htmlfunctions)) functions useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_linregress(t,y):\n",
    "    ''' Solve the system Am = d to find teh intercept and slope'''\n",
    "\n",
    "    # Solve Am = d\n",
    "    intercept, slope = \n",
    "\n",
    "    return intercept, slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call our function to calculate the intercept and slope of the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = my_linregress(time, height)\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your code from the last plot and add a line of code to plot the best-fit line through the data. Use ```plt.legend()``` to add a label for the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals or misfit ###\n",
    "The line looks like a reasonable representation of the data, but how did we do from a quantitative point of view? Let's compute the mean and standard deviation of the residual values (these are the values of the water depth minus the best fitting straight line through the data). Compute and print these values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is practically zero, which shows that underestimations of the observations are balanced by overestimations. The standard devation turns out to be close to 3.5 cm. Why do you think this is? What factors can you think of that contribute to the standard deviation? Type your responses in the box below: (Note, the scientists involved in collecting these data estimate the standard error in each of these annual means for sea level in Auckland is 2.5 cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Type your answers here*<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to feel that 3.5 cm is a \"poor\" fit, one could always fit the data better with a model that has more degrees of freedom. Does this experiment warrant a quadratic term? Or even higher-order polynomials? Maybe not over these 100 years of data, but if the rise is due to climate change and we have positive feedbacks, maybe we should account for that in our model. In any case:\n",
    "\n",
    "**Given enough degrees of freedom in the model, we can fit the (any) data perfectly!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression \"out of the box\"###\n",
    "\n",
    "By the way, there are many ways to do linear regression, or more advanced polynomial fitting, in Python. Here's one example of linear regression from the stats functions in scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(time, height)\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add this new best-fit line to the plot and confirm that it is identical to the previous line we calculated. Add the new line to the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decided to model the data with a polynomial instead of a straight line, then we could use the NumPy [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) function. Here, we fit with a third-degree polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(time, height, 3) #Least squares polynomial fit. Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). \n",
    "\n",
    "curve = (fit[0] * time ** 3) + (fit[1] * time ** 2) + (fit[2] * time) + fit[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate change? An exercise ###\n",
    "Australian scientists confirm their historic data also support a [1.6 mm/y rise in sea level](https://en.wikipedia.org/wiki/Sea_level_rise) averaged over the last 100+ years. However, tidal gauge and satellite data from the last decade(s) indicate sea level may now be rising at double this rate! With this info, have another look at the Auckland data. Most sea level values in the 2000s falls *above* the regression line. It would require more than data from one tidal gauge to attribute this significant, of course. Especially when you learn that the Auckland tidal gauge has been moved site three times since 2000. However, for the sake of a fitting exercise, write code to fit these data with a polynomial (see example above) and [an exponentional function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html). Plot the data with the linear, polynomial, and exponential fits on the same plot, and caluclate the residuals for each of the fits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the polynomial residuals smaller than for a linear fit? How about the exponential residuals? Are any of the residuals closer to the reported standard error in the data? What do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Type your answers here*<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
